{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb38d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading modules...\n",
      "Modules loaded\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The purpose of this code is to scan the titles of all pages within the TIF scanned documents. Our goal is to obtain exact\n",
    "page locations that have information that we would like to scrape.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print('Loading modules...', end='', flush=True)\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import fitz\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import difflib\n",
    "from difflib import SequenceMatcher\n",
    "import pandas as pd\n",
    "import math\n",
    "import sys\n",
    "import re\n",
    "print('\\nModules loaded')\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed857238",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORT_FOLDER = 'TIFpdfs'\n",
    "TOTAL_PAGES_TO_PARSE = 158021\n",
    "total_pages_parsed = 0\n",
    "total_files_parsed = 0\n",
    "\n",
    "# Check if there is a TIF folder\n",
    "if not os.path.isdir(REPORT_FOLDER) or not os.path.isdir(os.path.join(REPORT_FOLDER, '2001')):\n",
    "    print(f'There is no reports folder. Reports should be in \"{REPORT_FOLDER}\"')\n",
    "    sys.exit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ede849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\"\"\"\n",
    "Here we create a similarty function that returns a similarity percentage of two words, the first being\n",
    "one of the words from our word_list and the second being the OCR scanned word\n",
    "\"\"\"\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower())\n",
    "\n",
    "\n",
    "word_list = ['EXPENDITURES', 'balance', 'revenue', \"SCHEDULE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070875e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATABASE_FILE = 'page_loc.csv'\n",
    "COMPLETION_FILE = 'completed.json'\n",
    "\n",
    "# Check for pre-existing data\n",
    "if os.path.isfile(COMPLETION_FILE) and os.path.isfile(DATABASE_FILE):\n",
    "    # Try to read it in\n",
    "    try:\n",
    "        with open(COMPLETION_FILE, encoding='utf8') as data:\n",
    "            loaded_status = json.load(data)\n",
    "            print('Found data describing completion status, loading database...', end='', flush=True)\n",
    "\n",
    "            loaded_data = pd.read_csv(DATABASE_FILE)\n",
    "\n",
    "            print('Database loaded')\n",
    "\n",
    "            could_read = True\n",
    "\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        print(\"Could not load data file\")\n",
    "        could_read = False\n",
    "\n",
    "else:\n",
    "    print(\"No completion status\")\n",
    "    could_read = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dfa36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we couldn't read in the database but it does exist, we don't want to accidentally overwrite it\n",
    "if not could_read and (os.path.isfile(DATABASE_FILE) or os.path.isfile(COMPLETION_FILE)):\n",
    "    print('There is an existing database/completion file that could not be read')\n",
    "    print('Fix this or load a backup')\n",
    "    print('To prevent overwriting, this process will not continue')\n",
    "    sys.exit()\n",
    "\n",
    "completion_status = { }\n",
    "DATABASE_FIELDS = ['year', 'tif_number', 'page_num', 'block_num', 'par_num', 'line_num', 'word_num', 'left', 'top', 'width', 'height', 'conf', 'text']\n",
    "pandas_database = pd.DataFrame(columns=DATABASE_FIELDS)\n",
    "\n",
    "caught_up_to_last_savepoint = True\n",
    "\n",
    "if could_read:\n",
    "    pandas_database = loaded_data\n",
    "    completion_status = loaded_status\n",
    "    caught_up_to_last_savepoint = False\n",
    "    print('Catching up to last savepoint...', end='', flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79337b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk through all reports\n",
    "\n",
    "df_out = pd.DataFrame(columns=['year', 'tif_number', 'page_num', 'text'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for subdir, dirs, files in os.walk(REPORT_FOLDER):\n",
    "\n",
    "    year = subdir[8:]\n",
    "\n",
    "    if year not in completion_status:\n",
    "        completion_status[year] = {}\n",
    "\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        total_files_parsed += 1\n",
    "\n",
    "        if caught_up_to_last_savepoint:\n",
    "            print(f'Scanning {os.path.join(subdir, file)}')\n",
    "\n",
    "        if file not in completion_status[year]:\n",
    "            completion_status[year][file] = { 'successful': [], 'failed': [] }\n",
    "\n",
    "        try:\n",
    "            with fitz.open(os.path.join(subdir, file)) as pdf:\n",
    "\n",
    "                # Keep a buffer so we can add to the main dataframe in batches.\n",
    "                # Doing a bunch of small additions started taking a lot of time as\n",
    "                # the database got bigger.\n",
    "                buffer_database = pd.DataFrame(columns=DATABASE_FIELDS)\n",
    "\n",
    "                if not caught_up_to_last_savepoint and total_files_parsed % 40 == 0:\n",
    "                    print('.', end='', flush=True)\n",
    "\n",
    "                for page_number in range(len(pdf)):\n",
    "\n",
    "                    page_key = str(page_number)\n",
    "\n",
    "                    # * Un-comment later so we can fix errors that we ignored before\n",
    "                    # if page_key in completion_status[year][file]['successful']:\n",
    "                    if page_key in completion_status[year][file]['successful'] or page_key in completion_status[year][file]['failed']:\n",
    "                        # Maybe redundant but I might have fucked up somewhere\n",
    "                        if caught_up_to_last_savepoint:\n",
    "                            print(f'Already processed {page_key}')\n",
    "\n",
    "                        continue\n",
    "                    elif not caught_up_to_last_savepoint:\n",
    "                        print('Caught up!')\n",
    "                        print(f'Scanning {os.path.join(subdir, file)}')\n",
    "                        caught_up_to_last_savepoint = True\n",
    "                        \n",
    "#######################################################################################################################\n",
    "                    \"\"\"\"\"\n",
    "\n",
    "                    Here we are able to change the line :\n",
    "\n",
    "                        image = Image.frombytes(\"RGB\", [pixmap.width, 500], pixmap.samples)\n",
    "\n",
    "                    such that we only scan the title, which is roughly the top 20%, of a standard scanned TIF document\n",
    "\n",
    "                    \"\"\"\"\"\n",
    "                    page = pdf.load_page(page_number)\n",
    "                    pixmap = page.get_pixmap(dpi=200)\n",
    "                    image = Image.frombytes(\"RGB\", [pixmap.width, 500], pixmap.samples)\n",
    "                    image_bytes = io.BytesIO()\n",
    "                    image.save(image_bytes, jpg_quality=100, format=\"PNG\")\n",
    "                    #image.show()\n",
    "                    image_bytes.seek(0)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "                    try:\n",
    "                        orientation = pytesseract.image_to_osd(Image.open(image_bytes), output_type=pytesseract.Output.DICT)\n",
    "                        orientation['page_num'] = page_number + 1\n",
    "                    \n",
    "                        # I don't want to bother with rotation right now\n",
    "                        if orientation['rotate'] != 0:\n",
    "                            print(f'{page_number + 1} has non-0 orientation')\n",
    "                            if page_key not in completion_status[year][file]['failed']:\n",
    "                                completion_status[year][file]['failed'].append(page_key)\n",
    "\n",
    "                        else:\n",
    "                            page_df = pytesseract.image_to_data(Image.open(image_bytes), output_type=pytesseract.Output.DATAFRAME, lang='eng')\n",
    "\n",
    "                            # Columns we don't need\n",
    "                            page_df = page_df.drop(['page_num', 'level'], axis=1)\n",
    "\n",
    "                            # Get rid of empty values\n",
    "                            page_df = page_df.dropna(subset=['text'])\n",
    "\n",
    "                            # Low confidence and empty values\n",
    "                            page_df.drop(page_df[(page_df['conf'] < 3.0) | (page_df['text'] == ' ')].index, inplace=True)\n",
    "\n",
    "                            # Round the confidence\n",
    "                            page_df['conf'] = page_df['conf'].apply(lambda confidence: round(confidence, 2))\n",
    "                            \n",
    "#######################################################################################################################\n",
    "                            \"\"\"\"\"\n",
    "\n",
    "                            Here we check all the words for each block of text that has been scanned and see if any of the words include\n",
    "                            any of the keywords that we have. \n",
    "\n",
    "                            word_list = ['EXPENDITURES', 'balance', 'revenue', \"SCHEDULE\"]\n",
    "\n",
    "                            \"\"\"\"\"\n",
    "                            result = False\n",
    "                            for index, row in page_df.iterrows():\n",
    "                                word = row[\"text\"]\n",
    "                                for w in word_list:\n",
    "                                    similar_ratio = similar(w, word)\n",
    "                                    if similar_ratio.ratio() > 0.7:\n",
    "                                        result = True\n",
    "                                        \n",
    "                            if result == True:\n",
    "                                df_out = page_df.assign(year=year, tif_number=file[2:5], page_num=page_number)\n",
    "\n",
    "                            # print(page_df)\n",
    "\n",
    "                            # Append the items to our buffer database\n",
    "                            if buffer_database.empty:\n",
    "                                buffer_database = df_out\n",
    "                            else:\n",
    "                                buffer_database = pd.concat([buffer_database, df_out], copy=True, ignore_index=True)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "\n",
    "                            if page_key in completion_status[year][file]['failed']:\n",
    "                                completion_status[year][file]['failed'].remove(page_key)\n",
    "\n",
    "                            completion_status[year][file]['successful'].append(page_key)\n",
    "\n",
    "                    except pytesseract.pytesseract.TesseractError as tess_error:\n",
    "\n",
    "                        if tess_error.status == 1:\n",
    "                            print(f'{page_number + 1} has \"too few characters\"')\n",
    "                        else:\n",
    "                            print(f'{page_number + 1} has an unknown error')\n",
    "\n",
    "                        if page_key not in completion_status[year][file]['failed']:\n",
    "                            completion_status[year][file]['failed'].append(page_key)\n",
    "\n",
    "\n",
    "                    if page_number % 30 == 0 and len(pdf) - page_number >= 30 and page_number > 1:\n",
    "                        with open(COMPLETION_FILE, 'w', encoding='utf-8') as f:\n",
    "                            json.dump(completion_status, f, ensure_ascii=False)\n",
    "\n",
    " \n",
    "                            # Merge the buffer to the main database\n",
    "\n",
    "                            pandas_database = buffer_database.drop(columns=['block_num', 'par_num', 'line_num', 'word_num', 'left', 'top', 'width', 'height'])\n",
    "\n",
    "                            buffer_database = pd.DataFrame(columns=DATABASE_FIELDS)\n",
    "\n",
    "                            pandas_database.to_csv(DATABASE_FILE, index=False)\n",
    "\n",
    "                            print('Updated storage')\n",
    "\n",
    "                if caught_up_to_last_savepoint:\n",
    "                    with open(COMPLETION_FILE, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(completion_status, f, ensure_ascii=False)\n",
    "\n",
    "                        # Merge the buffer to the main database\n",
    "                        pandas_database = pd.concat([pandas_database, buffer_database], copy=False, ignore_index=True)\n",
    "                        buffer_database = pd.DataFrame(columns=DATABASE_FIELDS)\n",
    "\n",
    "                        pandas_database.to_csv(DATABASE_FILE, index=False)\n",
    "\n",
    "                print('Updated storage')\n",
    "\n",
    "                    total_pages_parsed += len(pdf)\n",
    "                    percent_through = total_pages_parsed / TOTAL_PAGES_TO_PARSE * 100\n",
    "                    percent_through = math.floor(percent_through)\n",
    "                    print(f'{percent_through:02d}% complete. {total_pages_parsed} pages parsed')\n",
    "                else:\n",
    "                    total_pages_parsed += len(pdf)\n",
    " \n",
    "            break\n",
    "        except fitz.FileDataError as e:\n",
    "            print(e)\n",
    "            print(f'Couldn\\'t open {os.path.join(subdir, file)}')\n",
    "\n",
    "\n",
    "            print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
