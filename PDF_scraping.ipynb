{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e87ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd932b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "                \n",
    "# Change years below\n",
    "\n",
    "linkBefore_2004 = \"https://www.chicago.gov/content/city/en/depts/dcd/supp_info/district-annual-reports--{}-.html\"\n",
    "linkBefore_2013 = \"https://www.chicago.gov/content/city/en/depts/dcd/supp_info/district_annual_reports{}.html\"\n",
    "linkAfter_2013 = \"https://www.chicago.gov/city/en/depts/dcd/supp_info/district-annual-reports--{}-.html\"\n",
    "start_year = 1997\n",
    "end_year = 2022\n",
    "years = [*range(start_year, end_year+1, 1)]\n",
    "\n",
    "urls = []\n",
    "\n",
    "# Generate and append URLs to the array\n",
    "for year in range(start_year, end_year + 1):\n",
    "    if year <= 2003:\n",
    "        urls.append(linkBefore_2004.format(year))\n",
    "    elif year <= 2012:\n",
    "        urls.append(linkBefore_2013.format(year))\n",
    "    elif year == 2022:\n",
    "        urls.append(\"https://www.chicago.gov/city/en/depts/dcd/supp_info/district-annual-reports--2022-2.html\")\n",
    "    elif year == 2016:\n",
    "        urls.append(\"https://www.chicago.gov/city/en/depts/dcd/supp_info/2016TIFAnnualReports.html\")\n",
    "    else:\n",
    "        urls.append(linkAfter_2013.format(year))\n",
    "\n",
    "\n",
    "    \n",
    "for index, url in enumerate(urls):\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7dcf51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function to check if the download was successful\n",
    "def check_broken(save_path):\n",
    "    try:\n",
    "        with fitz.open(save_path) as pdf:\n",
    "            if len(pdf) == 0:\n",
    "                return True\n",
    "\n",
    "    except fitz.FileDataError as file_error:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "# Function to download PDF file\n",
    "def download_pdf(url, save_path, ignore_corrupted=False):\n",
    "\n",
    "    # Check to see if we already downloaded it\n",
    "    if os.path.exists(save_path):\n",
    "        print(\"Already have, skipping download\")\n",
    "        return\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as pdf_file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                pdf_file.write(chunk)\n",
    "\n",
    "    # Check to see if the file is corrupted\n",
    "    is_broken = not ignore_corrupted and check_broken(pdf_name)\n",
    "\n",
    "    # Try again if broken\n",
    "    if is_broken:\n",
    "        print(f'Corrupted download of {full_url}, retrying')\n",
    "        os.remove(pdf_name)\n",
    "        download_pdf(full_url, pdf_name, True)\n",
    "\n",
    "        is_broken = check_broken(pdf_name)\n",
    "        if is_broken:\n",
    "            # Something fishy is going on\n",
    "            print(f'Cannot download {full_url}')\n",
    "            os.remove(pdf_name)\n",
    "\n",
    "\n",
    "# Loop through each URL\n",
    "for year, url in enumerate(urls):\n",
    "    print(f\"************************ Year {years[year]} Download Starting ************************\")\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract all links with .pdf extension\n",
    "    pdf_links = soup.find_all('a', href=lambda href: (href and href.endswith('.pdf')))\n",
    "\n",
    "    # Specify the directory to save PDF files\n",
    "    save_directory = 'TIFpdfs/'+str(years[year])\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "    # Download each PDF file\n",
    "    for pdf_link in pdf_links:\n",
    "        full_url = urljoin(url, pdf_link['href'])\n",
    "        pdf_name = os.path.join(save_directory, os.path.basename(full_url))\n",
    "        print(f\"Downloading: {full_url}\")\n",
    "        download_pdf(full_url, pdf_name)\n",
    "\n",
    "        \n",
    "    print(f\"************************ Year {years[year]} Download Complete ************************\")\n",
    "    \n",
    "    \n",
    "print(\"All Downloads complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1fd0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
